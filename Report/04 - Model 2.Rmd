---
title: "04 - Model 2"
author: "Hannah Harrison"
date: "2022-11-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model 2 

## Header content

Required libraries:
```{r}
if (!require("tsne")) install.packages("tsne")
if (!require("knitr")) install.packages("knitr")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("gridExtra")) install.packages("gridExtra")
if (!require("caret")) install.packages("caret")
if (!require("class")) install.packages("class")
if (!require("dplyr")) install.packages("dplyr")
if (!require("caTools")) install.packages("caTools")
if (!require("lsa")) install.packages("lsa")
if (!require("e1071")) install.packages('e1071')
if (!require("klaR")) install.packages('klaR')

library("tsne")
library("knitr")
library(ggplot2)
library(gridExtra)
library(caret)
library(class)
library(dplyr)
library(caTools)
library(lsa)
library(e1071)
library(klaR)
library(rpart)
library(rpart.plot)
```

Load the data:
```{r}
kdd_10_train =read.table(path_wd("..","data","kdd_10_train.tab"),header=TRUE)
```

##Introduction

In this script, I investigate using a Naive Bayes model to predict the class of attack. I then explore the effects of parameter tuning and feature augmentation using techniques such as PCA and scaling on the predictive performance, and select the best model based on this investigation.

## Initial exploration

We first take a look at the data, and check that it has been imported correctly.
```{r}
dim(kdd_10_train)
head(kdd_10_train)
```
We have a total of 444618 observations of 41 predictor variables, and two additional columns containing the attack type and class, or a label of normal if there is no attack. The objective of my model is to predict the attack class from the remaining features. 

### Data cleaning

Before any data analysis, it is important to check for and consider imputing any null values. 

```{r}
for (col in kdd_10_train){
  print(is.null(col))
}
kdd_10_train <- unique(kdd_10_train) #remove duplicate rows
kdd_10_train <- kdd_10_train[ - as.numeric(which(apply(kdd_10_train, 2, var) == 0))] #removing any zero variance features
dim(kdd_10_train)

```
There are no null values in any column, so we don't need to consider any imputation here, however we should proceed with caution, as with any network application, a value of 0 for a numeric measurement could indicate that the measurement process failed. 

We know that the columns contain the correct data types (factor/ numeric/string), as this was ensured in the data file.

### Initial EDA


```{r}
table(kdd_10_train$class)
prop.table(table(kdd_10_train$class))
plot(table(kdd_10_train$class))

```
Here we can see that most of the data (almost 60%) is normal network traffic. Of the data which corresponds to an attack, the vast majority are DDoS attacks, followed by Probe, R2L and UR2, which all correspond to relatively few observations. This dataset is highly imbalanced,and so we will need to be aware of this when building a model which needs to classify the points belonging to these different classes accurately.

## Model Fitting

I will further split the training data into two parts so that I can evaluate my models on left out data.

```{r}
set.seed(123)
kdd_10_train = kdd_10_train[, -40]
split = sample.split(kdd_10_train$class, SplitRatio = 0.75)
train_set = subset(kdd_10_train, split == TRUE)
test_set = subset(kdd_10_train, split == FALSE)
```


## Naive Bayes 

I now fit a Naive Bayes model. First, I define the features and labels to input into the model.

```{r}
train_label = train_set$class
train_features = train_set[ ,1:39]
test_label = test_set$class
test_features = test_set[,1:39]
```

Training the model using the naiveBayes function from the e1071 package.

```{r}
naive_bayes_model = naiveBayes(as.factor(train_label)~.,
                                    train_features)
```

Performance on the training set:
```{r}
# Prediction

naive_bayes_pred = predict(naive_bayes_model, train_features)

# Get the accuracy
NB_accuracy <- mean(train_label == naive_bayes_pred,na.rm = TRUE)
NB_accuracy

#Confusion matrix
cm <- confusionMatrix(as.factor(naive_bayes_pred),
                       as.factor(train_label))

cm
# Stats not so good for R2L; makes sense as this a small class
```
As we can see, the accuracy is 84%, which seems fairly good, however the predictions for R2L are pretty poor. It makes sense that the model would struggle with this class, as we identifies that it appears in relatively few data instances, however in this model, we correctly identify R2L attacks fewer than 1/3 of the time.

Performance on the left out data:
```{r}
# Prediction

naive_bayes_pred = predict(naive_bayes_model, test_features)

# Get the accuracy
NB_accuracy <- mean(test_label == naive_bayes_pred,na.rm = TRUE)
NB_accuracy


cm2 <- confusionMatrix(as.factor(naive_bayes_pred),
                       as.factor(test_label))
cm2
```
The results on the test set are similar, and we see an accuracy of 84% and similar problems with R2L. This is as expected, as the test and training set are from the same distribution, however it does provide a good sanity check that the model isn't too badly overfitted.


### Cross validation

We can further investigate the models out of sample performance using cross validation, and we can see whether this has the potential to improve the fit. Here I use the caret package, which facilitates the easy implementation of cross validation.


```{r}

# set up 5-fold cross validation procedure
train_control <- trainControl(
  method = "cv", 
  number = 5,
  savePredictions = TRUE
  )

# Train model
nb.m1 <- train(
  x = train_features,
  y = train_label,
  method = "nb",
  trControl = train_control
  )

# Confusion matrix
confusionMatrix(nb.m1)

```
Indeed, cross validation seems to have improved the model accuracy massively, as we have an average accuracy of 96%. How does this model fare on the test set?

```{r}
pred <- predict(nb.m1, newdata = test_features)
confusionMatrix(as.factor(pred), as.factor(test_label))
summary(pred)
```

Again looking at the average accuracy alone, the model appears to be performing well, however upon closer inspection of the predictions, we see that UR2 is never predicted and R2L is only predicted once, so the model isn't capturing the smaller classes at all! The model is predicting normal far too much.


### Parameter Tuning

Can the model fit be improved by tuning the hyperparameters?
The caret package also offers an easy way to tune parameters. In the naive bayes model, the hyperparameters we can tune are:
- the usekernel parameter, which allows us to use a kernel density estimate for continuous variables versus a gaussian density estimate,
- adjust, which allows us to adjust the bandwidth of the kernel density (larger numbers mean more flexible density estimate),
- fL, which allows us to incorporate the Laplace smoother

Rather than looking at all of these manually, which would take a long time and involve difficult comparisons, I will use the train function to consider the tuning of the parameters and output the best 5 models.

```{r}
# set up tuning grid
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:3,
  adjust = seq(0, 3, by = 1)
)

# train model
nb.m2 <- train(
  x = train_features,
  y = train_label,
  method = "nb",
  trControl = train_control,#cross validation
  tuneGrid = search_grid #tuning search grid
  )

# top 5 modesl
nb.m2$results %>% 
  top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))


# plot search grid results
plot(nb.m2)
```


Results for best model:

```{r}
confusionMatrix(nb.m2)
```

And on the left out set:
```{r}
pred <- predict(nb.m2, newdata = test_features)
confusionMatrix(as.factor(pred), as.factor(test_label))
```
We can see that the top 5 models all have similar accuracy to the previous model, and so tuning in this way doesn't seem to bring much improvement.

### Feature augmentation

The above code can be edited to also investigate the effects of transforming the features on the best model we have so far. Here I choose to try normalizing with Box Cox, standardizing with center-scaling, and reducing the dimensionality with PCA. 

```{r}

# train model
nb.m3 <- train(
  x = train_features,
  y = train_label,
  method = "nb",
  trControl = train_control, #cross validation
  preProc = c("BoxCox", "center", "scale", "pca") #set transformations
  )

# top 5 modesl
nb.m3$results %>% 
  top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))


# plot results
plot(nb.m3)
```

Results for best model:

```{r}
confusionMatrix(nb.m3)
```

And on the left out set:
```{r}
pred <- predict(nb.m3, newdata = test_features)
confusionMatrix(as.factor(pred), as.factor(test_label))
```
Whilst the accuracy has decreased slightly here, we can see from the confusion matrix that this model is much better at detecting R2L attacks. Hence, scaling has improved the detection rate for R2L at only a small cost to the accuracy. UR2 also at least now has some predictions, however as such a tiny class, it is still proving difficult to identify.

In summary, the fit of Naive Bayes models to this data can be improved by cross validation and scaling, and we can achieve a high model accuracy. Naive Bayes models struggle to identify the smaller classes, however this is improved by feature augmentation.

## Tree Based methods

I will now investigate some tree based methods, first using the package 'rpart' to create a cross validated decision tree, implementing the cross validation manually for illustration.

```{r}

# Set seed for reproducibility.
set.seed(1234)

# Define the number of folds.
k <- 5

# Generate folds.
folds <- sample(k, size = nrow(train_set), replace = TRUE)

# Print first 10 values.
head(folds)

```

The folds variable stores the fold each instance belongs to. For example, the first instance belongs to fold 4. We will iterate k =5 times.

Next, the rpart() function is used to train the decision tree with the train set. By default, rpart() performs  10 -fold cross-validation internally and so to avoid this, we set the parameter xval = 0. 

The ground truth classes and the predictions are stored so the performance metrics can be computed.

```{r}
# Variable to store ground truth classes.
groundTruth <- NULL

# Variable to store the classifier's predictions.
predictions <- NULL

for(i in 1:k){
  
  trainSet <- train_set[which(folds != i), ]
  testSet <- train_set[which(folds == i), ]
  
  # Train the decision tree
  treeClassifier <- rpart(class ~ .,
                          trainSet, xval=0)
  
  # Get predictions on the test set.
  foldPredictions <- predict(treeClassifier,
                             testSet, type = "class")
  
  predictions <- c(predictions,
                   as.character(foldPredictions))
  
  groundTruth <- c(groundTruth,
                   as.character(testSet$class))
}

cm <- confusionMatrix(as.factor(predictions),
                      as.factor(groundTruth))


cm$overall["Accuracy"]

cm
```


```{r}
# Plot the tree from the last fold.
rpart.plot(treeClassifier, fallen.leaves = F,
           shadow.col = "gray", legend.y = 1)
```

### Xgboost

I now implement a boosted decision tree using xgboost.

The xgboost model takes...

```{r}
train <- training_set

for (i in c(2,3,4, 42)){
  train[,i] = as.numeric(as.factor(train[,i]))
}
summary(train)

train$class = train$class - 1
summary(train)
```


```{r}
#making xgboost matrix

data_label <- train[,"class"]
data_matrix <- xgb.DMatrix(data = as.matrix(train), label = data_label)
```


```{r}
#CV
numberOfClasses <- length(unique(train$class))
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = numberOfClasses)
nround    <- 50 # number of XGBoost rounds
cv.nfold  <- 5
```


```{r}
# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
                   data = data_matrix, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = FALSE,
                   prediction = TRUE)
```


```{r}
# Prediction
OOF_prediction <- data.frame(cv_model$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"),
         label = data_label + 1)
head(OOF_prediction)

confusionMatrix(factor(OOF_prediction$max_prob),
                factor(OOF_prediction$label),
                mode = "everything")

```

Left out set:
```{r}

```



---
title: "04 - Model 2"
author: "Hannah Harrison"
date: "2022-11-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model 2 - Feature Augmentation

## Header content

Required libraries:
```{r}
if (!require("tsne")) install.packages("tsne")
if (!require("knitr")) install.packages("knitr")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("gridExtra")) install.packages("gridExtra")
if (!require("caret")) install.packages("caret")
if (!require("class")) install.packages("class")
if (!require("dplyr")) install.packages("dplyr")
if (!require("caTools")) install.packages("caTools")
if (!require("lsa")) install.packages("lsa")

library("tsne")
library("knitr")
library(ggplot2)
library(gridExtra)
library(caret)
library(class)
library(dplyr)
library(caTools)
library(lsa)

```

Load the data:
```{r}
kdd_10_train =read.table(path_wd("..","data","kdd_10_train.tab"),header=TRUE)
```

##Introduction

In this script, I explore the effects of augmenting the feature space using techniques such as PCA and clustering on the predictive performance, and build a model based on this investigation.

## Initial exploration

We first take a look at the data, and check that it has been imported correctly.
```{r}
dim(kdd_10_train)
head(kdd_10_train)
```
We have a total of 444618 observations of 41 predictor variables, and two additional columns containing the attack type and class, or a label of normal if there is no attack. The objective of my model is to predict the attack class from the remaining features. 

### Data cleaning

Before any data analysis, it is important to check for and consider imputing any null values. 

```{r}
for (col in kdd_10_train){
  print(is.null(col))
}
kdd_10_train <- unique(kdd_10_train) #remove duplicate rows
dim(kdd_10_train)

# Clean up near zero variance features
nzvcol <- nearZeroVar(kdd_10_train)
kdd_10_train <- kdd_10_train[, -nzvcol]
```
There are no null values in any column, so we don't need to consider any imputation here, however we should proceed with caution, as with any network application, a value of 0 for a numeric measurement could indicate that the measurment process failed. 

We know that the columns contain the correct data types (factor/ numeric/string), as this was ensured in the data file.

### Initial EDA


```{r}
table(kdd_10_train$class)
prop.table(table(kdd_10_train$class))
plot(table(kdd_10_train$class))

```
Here we can see that most of the data (almost 60%) is normal network traffic. Of the data which corresponds to an attack, the vast majority are DDoS attacks, followed by Probe, R2L and UR2, which all correspond to relatively few observations. This dataset is highly imbalanced,and so we will need to be aware of this when building a model which needs to classify the points belonging to these different classes accurately.

## Model comparison

### Naive Bayes 


### KNN

Since knn uses a distance measure, we first need to convert the character variables to numeric. Since there is no natural ordering to protocol_type, service or flag, we will need to use one-hot encoding. I will do this using the dummyVars functionn from the caret package.

```{r}
dummy <- dummyVars(" ~ .", data=kdd_10_train[, 1:41])

#perform one-hot encoding on data frame
kdd_10_train_1 <- cbind(data.frame(predict(dummy, newdata=kdd_10_train[ , 1:41])), kdd_10_train[, 42:43])
kdd_10_train_1 %>% distinct()

head(kdd_10_train_1)
```
We can see that there are now columns for each value of protocol_type, service and flag. I also select only the unique rows of the dataset so that the nearest neighbours are distinct. The problem here is that we now have a huge number of features and so KNN is very slow, as there are a lot of dimensions. Hence I will simply remove protocol_type as this contributes 66 of the additional features. 

```{r}
unique(kdd_10_train$service)
unique(kdd_10_train$protocol_type)
unique(kdd_10_train$flag)
```

```{r}
dummy <- dummyVars(" ~ .", data=cbind(kdd_10_train[,1:2], kdd_10_train[, 5:41]))

#perform one-hot encoding on data frame
kdd_10_train_1 <- cbind(data.frame(predict(dummy, newdata=cbind(kdd_10_train[,1:2], kdd_10_train[, 5:41]))), kdd_10_train[, 42:43])
kdd_10_train_1 %>% distinct()

head(kdd_10_train_1)
```
This leaves us with 41 predictors.


I split the training set for cross validation.
```{r}
set.seed(123)
split = sample.split(kdd_10_train_1$class, SplitRatio = 0.75)
training_set = subset(kdd_10_train_1, split == TRUE)
test_set = subset(kdd_10_train_1, split == FALSE)
```


### Feature Scaling

It is also important to scale the data for KNN, again due to the distance metric. I will try two methods, standardization and min/max scaling and compare the outputs.

Standardisation:
```{r}
train_feat <- cbind(training_set[,1], training_set[ ,5:41])
standard_scaler <- preProcess(train_feat, method=c('center', 'scale'))
train_s_sc <- predict(standard_scaler, train_feat)

summary(train_s_sc)
```

We now implement KNN with k = 3 and view the first 10 predictions. 
```{r}
set.seed(1)
train_pred <- knn(train_s_sc, train_s_sc, training_set$class, k=1)

train_pred[1:10]
```

Calculating the accuracy:
```{r}
accuracy <- mean(train_pred == kdd_10_train$class)
cat("Training Accuracy: ", accuracy, sep='')
```

Maybe try a different distance metric such as cosine.


